{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFwUzgFbFe2n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import json\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/cbio_longitudinal_v2.csv')"
      ],
      "metadata": {
        "id": "7fzlNlyUFvpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_nan = df.dropna(how='any')\n",
        "df_no_nan"
      ],
      "metadata": {
        "id": "vSiSvqTYF-Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_nan = df[df.isna().any(axis=1)]\n",
        "df_with_nan"
      ],
      "metadata": {
        "id": "ao_vMchNGHS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_with_nan = df_with_nan.columns[df.isna().any()].tolist()\n",
        "print(cols_with_nan)"
      ],
      "metadata": {
        "id": "v9b93uQVJsth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_qwen_format(\n",
        "    csv_path: str,\n",
        "    target_columns = None,\n",
        "    output_dir: str = \"qwen_data\",\n",
        "    mask_ratio: float = 0.3,\n",
        "    train_ratio: float = 0.7,\n",
        "    val_ratio: float = 0.15,\n",
        "    test_ratio: float = 0.15,\n",
        "    system_prompt: str = \"You are a data imputation assistant that predicts missing values based on available features.\",\n",
        "    random_seed: int = 42\n",
        "):\n",
        "\n",
        "    random.seed(random_seed)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    initial_rows = len(df)\n",
        "    df = df.dropna()\n",
        "    removed_rows = initial_rows - len(df)\n",
        "\n",
        "    training_examples = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "\n",
        "        row_dict = row.to_dict()\n",
        "\n",
        "        target_cols_available = [col for col in target_columns if col in row_dict.keys()]\n",
        "        feature_cols = [col for col in row_dict.keys() if col not in target_columns]\n",
        "\n",
        "        if len(target_cols_available) < 1:\n",
        "            continue\n",
        "\n",
        "        n_mask = max(1, int(len(target_cols_available) * mask_ratio))\n",
        "        cols_to_mask = random.sample(target_cols_available, n_mask)\n",
        "\n",
        "        context_parts = []\n",
        "        target_parts = []\n",
        "\n",
        "        for col in feature_cols:\n",
        "            context_parts.append(f\"{col}: {row_dict[col]}\")\n",
        "\n",
        "        for col in target_cols_available:\n",
        "            if col in cols_to_mask:\n",
        "                context_parts.append(f\"{col}: [MASK]\")\n",
        "                target_parts.append(f\"{col}: {row_dict[col]}\")\n",
        "            else:\n",
        "                context_parts.append(f\"{col}: {row_dict[col]}\")\n",
        "\n",
        "        user_content = f\"Complete the missing values: {', '.join(context_parts)}\"\n",
        "        assistant_content = \", \".join(target_parts)\n",
        "\n",
        "        example = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_content},\n",
        "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        training_examples.append(example)\n",
        "\n",
        "    n = len(training_examples)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = train_end + int(n * val_ratio)\n",
        "\n",
        "    splits = {\n",
        "        'train': training_examples[:train_end],\n",
        "        'val': training_examples[train_end:val_end],\n",
        "        'test': training_examples[val_end:]\n",
        "    }\n",
        "\n",
        "\n",
        "    for split_name, split_data in splits.items():\n",
        "        output_file = f\"/content/{split_name}.jsonl\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for example in split_data:\n",
        "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"   ✅ {split_name:5s}: {len(split_data):5d} exemplos → {output_file}\")\n",
        "\n",
        "    return splits"
      ],
      "metadata": {
        "id": "6bFD_GkhZH-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_to_qwen_format(\n",
        "        csv_path=\"/content/cbio_longitudinal_v2.csv\",\n",
        "        output_dir=\"qwen_data\",\n",
        "        mask_ratio=0.3,\n",
        "        train_ratio=0.7,\n",
        "        val_ratio=0.15,\n",
        "        test_ratio=0.15,\n",
        "        random_seed=42,\n",
        "        target_columns=cols_with_nan\n",
        "    )"
      ],
      "metadata": {
        "id": "9CeUTfukZcJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}